# CNN SAM2

To prepare the data for training and evaluation, we first used the `sam2-pipeline.py` script to split the DYB-PlanktonNet dataset into three parts: training, validation, and testing. These were saved in the folders `sam2_train_masks_yes`, `sam2_val_masks_yes`, and `sam2_test_masks_yes` accordingly. Each dataset contains images of plankton that have been cropped using masks generated by the SAM2 model, ensuring that the full object is included in each crop.

Next, we calculated the average (mean) and standard deviation (std) of the training images using the `09_mean_std_variables.py` script. These statistics help normalize the images before training the model.

For model training, we first trained the backbone (feature extractor) and then selected the best-performing backbone weights by running the `main_linear_loop_sam2.sh` script. Finally, we evaluated the model's accuracy on the test set using the `main_check_accuracies_sam2.py` script. To systematically analyze the impact of feature redundancy and class imbalance on similarity-based classification, we developed a pipeline of scripts:

**Feature Extraction and Storage**
- **`10_lut_creation_sam2.py`**: Extracts feature vectors from the training images using the trained backbone and stores them in a FAISS index, while also recording the mapping between each feature and its class label in a SQLite database.

**Feature Pruning and Redundancy Reduction**
- **`11_lut_pruning_sam2.py`**: Prunes redundant features from the FAISS index and database by removing vectors that are too similar (above a configurable cosine similarity threshold) within each class, producing a reduced lookup table.

**Inference and Evaluation**
- **`12_inference_sam2.py`**: Performs nearest neighbor inference on the test set using the pruned FAISS index and database, reporting accuracy and other performance metrics.

**Threshold Experimentation**
- **`13_run_threshold_experiments_sam2.py`**: Automates the process of running the pruning and inference steps for a range of similarity thresholds, saving the results (including accuracy, processing time, FAISS index size, and class imbalance metrics) to a JSON file.

**Visualization and Analysis**
- **`14_thresholds_comparison_sam2.py`**: Completes the pipeline by visualizing the results from the threshold experiments, creating a comprehensive multi-axis plot that compares accuracy, processing time, FAISS index size, and optionally the Gini coefficient (class imbalance metric) across different similarity thresholds.

This systematic approach enables a thorough evaluation of the trade-offs between redundancy, accuracy, and class balance in the LUT-based classification approach.

## Training

### Methodology

The backbone training is performend on a fixed number of epochs (1000). Later, we observe the loss decay via tensorboard, and pick the saved epochs where we start seeing a loss' plateau (in our case, from xxx to last). Next, we train a linear head over each one of these epochs while monitoring the validation performance. Thus, we select the weights were the validation accuracy is the highest, and with these weights, we evaluate the performance of the linear head using the testing set.

For DYB-linearHead dataset, **batch size 32**:

1. Backbone training (features extractor), using **ResNet50**. Elapsed training time ? days, ? hours, ? min. Use of memory: ?MiB / ?MiB (??.??%). Expired e-mail password token, thus no notification received.

- The learning rate of 0.016 was selected as a linear relation between the original batch size of 1024 and learning rate of 0.5; thus dividing 0.5/32 = 0.0156 rounded to 0.016. 

```bash
python3 main_supcon.py --batch_size 32 --num_workers 8 --learning_rate 0.016 --lr_decay_epochs 10 --temp 0.07 --cosine --mean "0.0425, 0.0436, 0.0432" --std "0.1466, 0.1488, 0.1476" --dataset path --data_folder /home/dsosatr/ofa/segment-anything/output/sam2_train_masks_yes --size 224
```

2. Head training (classifier). It is necessary to iterate over every backbone weights (in our case we iterate from epoch 700 to last, so we place the rest of these backbone weights into a folder named `discarded_for_head_training`):

```bash
./main_linear_loop_sam2.sh
```

---

### Results

Resnet50timm, backbone batch_size = 32, learning_rate = 0.016, employed dataset `sam2_train_masks_yes`, head trained using the frozen backbone on each epoch and validated with the `sam2_val_masks_yes` folder using a bs = 256 and learning_rate = 2.5. 04 jun 2025.

|             	| ResNet50               	|
|-------------	|------------------------	|
| ckpt number 	| ACC bs=32 / head epoch 	|
| last.pth    	| 91.92% / 67            	|
| 1000.pth    	| 91.92% / 46            	|
| 950.pth     	| 92.19% / 87            	|
| 900.pth     	| 91.92% / 73            	|
| 850.pth     	| 91.95% / 48            	|
| 800.pth     	| 91.88% / 60            	|
| 750.pth     	| 91.46% / 65            	|
| 700.pth     	| 91.61% / 61            	|

Finally, we test the accuracy of the backbone using the following batch sizes at epoch (see column "bs / backbone epoch / head epoch"), with the `sam2_test_masks_yes` dataset.

| bs / backbone epoch / head epoch 	| Accuracy 	| Precision 	| Recall 	| F1 Score 	| Network  	|
|----------------------------------	|----------	|-----------	|--------	|----------	|----------	|
| 32 / 950 / 87                    	| 91.23%   	| 86.12%    	| 79.26% 	| 81.07%   	| ResNet50 	|

The above table was obtained using the following script, where we changed the input arguments model, val_folder, among others.
```bash
python3 main_check_accuracies_sam2.py
```

## Notes regarding training

See notes in [CNN Model, stage 1](cnn_model.md).